{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fc5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa87750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"11.csv\")\n",
    "\n",
    "# Separate data into different dataframes based on data types\n",
    "spo2_data = data[data['data_type'] == 'spo2'].copy()\n",
    "heart_rate_data = data[data['data_type'] == 'hr'].copy()\n",
    "steps_data = data[data['data_type'] == 'steps'].copy()\n",
    "hrv_data = data[data['data_type'] == 'hrv'].copy()\n",
    "br_data = data[data['data_type'] == 'br'].copy()\n",
    "sleep_data = data[data['data_type'] == 'sleep'].copy()\n",
    "\n",
    "# Convert timestamps to datetime format for each dataframe\n",
    "spo2_data['time'] = pd.to_datetime(spo2_data['time'])\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time'])\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time'])\n",
    "hrv_data['time'] = pd.to_datetime(hrv_data['time'])\n",
    "br_data['time'] = pd.to_datetime(br_data['time'])\n",
    "sleep_data['time'] = pd.to_datetime(sleep_data['time'])\n",
    "\n",
    "# Sort dataframes based on time and their respective formats\n",
    "spo2_data.sort_values(by='time', inplace=True)\n",
    "heart_rate_data.sort_values(by='time', inplace=True)\n",
    "steps_data.sort_values(by='time', inplace=True)\n",
    "hrv_data.sort_values(by='time', inplace=True)\n",
    "br_data.sort_values(by='time', inplace=True)\n",
    "sleep_data.sort_values(by='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c06502",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data=heart_rate_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3062db4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>2023-12-04 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>2023-12-04 00:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>2023-12-04 00:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83</td>\n",
       "      <td>2023-12-04 00:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>79</td>\n",
       "      <td>2023-12-04 00:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>87</td>\n",
       "      <td>2023-12-23 13:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>85</td>\n",
       "      <td>2023-12-23 13:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>85</td>\n",
       "      <td>2023-12-23 13:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>86</td>\n",
       "      <td>2023-12-23 13:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>91</td>\n",
       "      <td>2023-12-23 13:04:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1777 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     value                      time\n",
       "1       80 2023-12-04 00:00:00+00:00\n",
       "3       82 2023-12-04 00:01:00+00:00\n",
       "4       82 2023-12-04 00:02:00+00:00\n",
       "5       83 2023-12-04 00:03:00+00:00\n",
       "6       79 2023-12-04 00:04:00+00:00\n",
       "...    ...                       ...\n",
       "2539    87 2023-12-23 13:00:00+00:00\n",
       "2540    85 2023-12-23 13:01:00+00:00\n",
       "2541    85 2023-12-23 13:02:00+00:00\n",
       "2542    86 2023-12-23 13:03:00+00:00\n",
       "2543    91 2023-12-23 13:04:00+00:00\n",
       "\n",
       "[1777 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_rate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1f459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_data=steps_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293fadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-12-04 00:58:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-12-04 00:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-12-04 01:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-12-04 01:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>22</td>\n",
       "      <td>2023-12-04 01:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-12-22 15:41:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>12</td>\n",
       "      <td>2023-12-22 15:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-12-22 15:47:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-12-22 15:48:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-12-23 13:06:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     value                      time\n",
       "60       9 2023-12-04 00:58:00+00:00\n",
       "63       9 2023-12-04 00:59:00+00:00\n",
       "65      13 2023-12-04 01:00:00+00:00\n",
       "67       7 2023-12-04 01:01:00+00:00\n",
       "69      22 2023-12-04 01:02:00+00:00\n",
       "...    ...                       ...\n",
       "2518     8 2023-12-22 15:41:00+00:00\n",
       "2522    12 2023-12-22 15:44:00+00:00\n",
       "2527    18 2023-12-22 15:47:00+00:00\n",
       "2528     6 2023-12-22 15:48:00+00:00\n",
       "2544    13 2023-12-23 13:06:00+00:00\n",
       "\n",
       "[437 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ee195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Features Data:\n",
      "            timestamp                                   heart_rate_means  \\\n",
      "0 2023-12-04 13:23:55  [-0.2351159491584021, 2.1804588709621635, -1.1...   \n",
      "1 2023-12-05 01:15:02  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2 2023-12-05 21:01:18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3 2023-12-06 08:20:31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4 2023-12-08 13:05:46  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                         steps_means  \n",
      "0  [-1.284156719038521, -2.555416598717975, -2.17...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "Non-Use Features Data:\n",
      "            timestamp                                   heart_rate_means  \\\n",
      "0 2023-12-09 09:37:20  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "1 2023-12-10 11:02:50  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2 2023-12-13 11:09:21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3 2023-12-16 10:15:22  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4 2023-12-17 12:50:33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                         steps_means  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load use and non-use data\n",
    "use_data = pd.read_csv('ID11_Use.csv')\n",
    "non_use_data = pd.read_csv('ID11_None.csv')\n",
    "\n",
    "# Filter rows with 'Melon' in the substance_fruit_label column\n",
    "use_data = use_data[use_data['substance_fruit_label'] == 'Nectarine']\n",
    "\n",
    "# Standardize the timestamp format in 'hawaii_use_time'\n",
    "use_data['hawaii_use_time'] = use_data['hawaii_use_time'].apply(lambda x: x.split('.')[0] if '.' in x else x)\n",
    "use_data['hawaii_use_time'] = pd.to_datetime(use_data['hawaii_use_time'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)\n",
    "\n",
    "# Convert timestamps to ensure they are timezone-naive for non-use data\n",
    "non_use_data['hawaii_createdat_time'] = pd.to_datetime(non_use_data['hawaii_createdat_time'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)\n",
    "\n",
    "# Ensure heart rate and steps data use the same timezone-naive datetime index\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time']).dt.tz_localize(None)\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time']).dt.tz_localize(None)\n",
    "\n",
    "heart_rate_data.set_index('time', inplace=True)\n",
    "steps_data.set_index('time', inplace=True)\n",
    "\n",
    "# Convert the 'value' column to numeric, coercing errors\n",
    "heart_rate_data['value'] = pd.to_numeric(heart_rate_data['value'], errors='coerce')\n",
    "steps_data['value'] = pd.to_numeric(steps_data['value'], errors='coerce')\n",
    "\n",
    "# Remove duplicate timestamps\n",
    "heart_rate_data = heart_rate_data[~heart_rate_data.index.duplicated(keep='first')]\n",
    "steps_data = steps_data[~steps_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Reindex and forward fill to handle missing data\n",
    "heart_rate_data = heart_rate_data.reindex(pd.date_range(start=heart_rate_data.index.min(), end=heart_rate_data.index.max(), freq='min'), method='ffill')\n",
    "steps_data = steps_data.reindex(pd.date_range(start=steps_data.index.min(), end=steps_data.index.max(), freq='min'), method='ffill').fillna(0)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def process_label_data(label_data, heart_rate_data, steps_data, scaler, time_column):\n",
    "    results = []\n",
    "    for _, row in label_data.iterrows():\n",
    "        timestamp = row[time_column]\n",
    "        lower_bound = timestamp - pd.Timedelta(hours=1)\n",
    "        upper_bound = timestamp + pd.Timedelta(hours=1)\n",
    "\n",
    "        # Extract data within the window\n",
    "        hr_window = heart_rate_data.loc[lower_bound:upper_bound]\n",
    "        steps_window = steps_data.loc[lower_bound:upper_bound]\n",
    "\n",
    "        # Resample and calculate mean every 4 minutes to get 30 points\n",
    "        hr_means = hr_window.resample('4min').mean().iloc[:30]  # Ensure 30 data points\n",
    "        steps_means = steps_window.resample('4min').mean().iloc[:30]\n",
    "\n",
    "        if len(hr_means) == 30 and len(steps_means) == 30:\n",
    "            # Standardize the means\n",
    "            hr_scaled = scaler.fit_transform(hr_means.values.reshape(-1, 1))\n",
    "            steps_scaled = scaler.fit_transform(steps_means.values.reshape(-1, 1))\n",
    "\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'heart_rate_means': hr_scaled.flatten().tolist(),\n",
    "                'steps_means': steps_scaled.flatten().tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process use and non-use labels\n",
    "use_features = process_label_data(use_data, heart_rate_data, steps_data, scaler, 'hawaii_use_time')\n",
    "non_use_features = process_label_data(non_use_data, heart_rate_data, steps_data, scaler, 'hawaii_createdat_time')\n",
    "\n",
    "# Output results\n",
    "print(\"Use Features Data:\")\n",
    "print(use_features.head())\n",
    "print(\"Non-Use Features Data:\")\n",
    "print(non_use_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc085be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Use Features Data: (20, 3)\n",
      "Shape of Non-Use Features Data: (5, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the use features data\n",
    "print(\"Shape of Use Features Data:\", use_features.shape)\n",
    "\n",
    "# Print the shape of the non-use features data\n",
    "print(\"Shape of Non-Use Features Data:\", non_use_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdfafd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp                                   heart_rate_means  \\\n",
      "0  2023-12-04 13:23:55  [-0.2351159491584021, 2.1804588709621635, -1.1...   \n",
      "1  2023-12-05 01:15:02  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  2023-12-05 21:01:18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3  2023-12-06 08:20:31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  2023-12-08 13:05:46  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "5  2023-12-08 16:49:22  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "6  2023-12-08 21:29:21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "7  2023-12-09 09:37:20  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "8  2023-12-10 02:41:28  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "9  2023-12-10 11:02:38  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "10 2023-12-10 11:02:50  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "11 2023-12-11 00:29:08  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "12 2023-12-11 16:59:33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "13 2023-12-12 00:29:30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "14 2023-12-12 20:35:51  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "15 2023-12-13 08:30:18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "16 2023-12-13 11:08:18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "17 2023-12-13 11:08:18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "18 2023-12-13 11:09:21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "19 2023-12-14 21:10:59  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "20 2023-12-15 08:30:52  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "21 2023-12-15 20:55:31  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "22 2023-12-16 10:15:22  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "23 2023-12-17 12:50:33  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "24 2023-12-21 14:15:29  [-0.7506597779113109, -0.7506597779113109, -0....   \n",
      "\n",
      "                                          steps_means      state  state_val  \n",
      "0   [-1.284156719038521, -2.555416598717975, -2.17...        use          1  \n",
      "1   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "2   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "3   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "4   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "5   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "6   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "7   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  non_crave          0  \n",
      "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "10  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  non_crave          0  \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "13  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "14  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "15  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "16  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "17  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "18  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  non_crave          0  \n",
      "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "20  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        use          1  \n",
      "22  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  non_crave          0  \n",
      "23  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  non_crave          0  \n",
      "24  [0.6015652739092174, 0.6015652739092174, 0.601...        use          1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adding label columns\n",
    "use_features['state'] = 'use'\n",
    "use_features['state_val'] = 1\n",
    "\n",
    "non_use_features['state'] = 'non_crave'\n",
    "non_use_features['state_val'] = 0\n",
    "\n",
    "# Combining the dataframes\n",
    "combined_data = pd.concat([use_features, non_use_features])\n",
    "\n",
    "# Converting timestamps to datetime and sorting\n",
    "combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'])\n",
    "combined_data_sorted = combined_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# This combined_data_sorted is now ready for use in your neural network model\n",
    "print(combined_data_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81aa308",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at heart_rate_encoder",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load pretrained models\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m encoder_hr \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheart_rate_encoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m encoder_steps \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    255\u001b[0m         filepath,\n\u001b[0;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         )\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    241\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at heart_rate_encoder"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained models\n",
    "encoder_hr = tf.keras.models.load_model('heart_rate_encoder')\n",
    "encoder_steps = tf.keras.models.load_model('steps_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d33482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to generate embeddings using the loaded models\n",
    "def generate_embeddings(hr_data, steps_data, encoder_hr, encoder_steps):\n",
    "    # Convert lists to numpy arrays and ensure they are in the correct shape\n",
    "    hr_data = np.array(hr_data.tolist())\n",
    "    steps_data = np.array(steps_data.tolist())\n",
    "\n",
    "    # Reshape data if required by the model, assuming the model expects shape (samples, features)\n",
    "    if hr_data.ndim == 1:\n",
    "        hr_data = hr_data.reshape(-1, 1)\n",
    "    if steps_data.ndim == 1:\n",
    "        steps_data = steps_data.reshape(-1, 1)\n",
    "\n",
    "    # Generate embeddings\n",
    "    hr_embeddings = encoder_hr.predict(hr_data)\n",
    "    steps_embeddings = encoder_steps.predict(steps_data)\n",
    "\n",
    "    return hr_embeddings, steps_embeddings\n",
    "\n",
    "# Use the function with your data\n",
    "hr_embeddings, steps_embeddings = generate_embeddings(\n",
    "    combined_data_sorted['heart_rate_means'],\n",
    "    combined_data_sorted['steps_means'],\n",
    "    encoder_hr,\n",
    "    encoder_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ffada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Combine the heart rate and steps embeddings\n",
    "combined_embeddings = np.concatenate([hr_embeddings, steps_embeddings], axis=1)\n",
    "\n",
    "# Labels from your existing DataFrame\n",
    "labels = combined_data_sorted['state_val'].values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with regularization and batch normalization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate thresholds on training set\n",
    "probabilities_train = model.predict(X_train)\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Store training results\n",
    "training_results = []\n",
    "\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_train > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    training_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert training results to DataFrame for easy viewing\n",
    "training_results_df = pd.DataFrame(training_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Select best threshold based on a specific criterion\n",
    "# Here, we select the threshold with the highest sensitivity while keeping specificity above 0.5\n",
    "best_threshold_row = training_results_df[(training_results_df['Sensitivity'] > 0.9) & (training_results_df['Specificity'] > 0.5)]\n",
    "if not best_threshold_row.empty:\n",
    "    best_threshold = best_threshold_row.iloc[0]['Threshold']\n",
    "else:\n",
    "    best_threshold = 0.5  # Default threshold if criteria is not met\n",
    "\n",
    "print(f\"\\nBest Threshold from Training Set: {best_threshold}\")\n",
    "\n",
    "# Evaluate the best threshold on the test set\n",
    "probabilities_test = model.predict(X_test)\n",
    "predicted_classes_test = (probabilities_test > best_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "recall_test = tp / (tp + fn)  # Sensitivity\n",
    "specificity_test = tn / (tn + fp)  # Specificity\n",
    "accuracy_test = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"\\nTest Set Evaluation at Best Threshold ({best_threshold}):\")\n",
    "print(f\"Sensitivity: {recall_test:.2f}\")\n",
    "print(f\"Specificity: {specificity_test:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.2f}\")\n",
    "\n",
    "# Store test results\n",
    "test_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_test > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    test_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert test results to DataFrame for easy viewing\n",
    "test_results_df = pd.DataFrame(test_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Find the threshold where specificity is around 0.9 and sensitivity is around 0.5\n",
    "desired_specificity = 0.9\n",
    "desired_sensitivity_range = (0.4, 0.6)  # Considering sensitivity around 0.5\n",
    "\n",
    "filtered_results = test_results_df[\n",
    "    (test_results_df['Specificity'] >= desired_specificity) & \n",
    "    (test_results_df['Sensitivity'] >= desired_sensitivity_range[0]) & \n",
    "    (test_results_df['Sensitivity'] <= desired_sensitivity_range[1])\n",
    "]\n",
    "\n",
    "# Print the filtered results\n",
    "print(\"\\nFiltered Thresholds with Specificity >= 0.9 and Sensitivity ~ 0.5:\")\n",
    "print(filtered_results)\n",
    "\n",
    "# Plot the results for better visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Sensitivity'], label='Sensitivity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Specificity'], label='Specificity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Accuracy'], label='Accuracy')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='Desired Specificity (0.9)')\n",
    "plt.axhline(y=0.5, color='g', linestyle='--', label='Desired Sensitivity (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity, Specificity, and Accuracy vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training\n",
    "Threshold: 0.45, Sensitivity: 0.90, Specificity: 0.57, Accuracy: 0.80\n",
    "Threshold: 0.46, Sensitivity: 0.87, Specificity: 0.83, Accuracy: 0.85\n",
    "Threshold: 0.47, Sensitivity: 0.75, Specificity: 0.87, Accuracy: 0.79\n",
    "Threshold: 0.48, Sensitivity: 0.73, Specificity: 0.96, Accuracy: 0.80\n",
    "Threshold: 0.49, Sensitivity: 0.69, Specificity: 0.96, Accuracy: 0.77\n",
    "Threshold: 0.50, Sensitivity: 0.67, Specificity: 0.96, Accuracy: 0.76\n",
    "Threshold: 0.51, Sensitivity: 0.65, Specificity: 0.96, Accuracy: 0.75\n",
    "Threshold: 0.52, Sensitivity: 0.62, Specificity: 0.96, Accuracy: 0.72\n",
    "Threshold: 0.53, Sensitivity: 0.62, Specificity: 0.96, Accuracy: 0.72\n",
    "Threshold: 0.54, Sensitivity: 0.56, Specificity: 1.00, Accuracy: 0.69\n",
    "Threshold: 0.55, Sensitivity: 0.48, Specificity: 1.00, Accuracy: 0.64\n",
    "Threshold: 0.56, Sensitivity: 0.44, Specificity: 1.00, Accuracy: 0.61\n",
    "Test:\n",
    "Threshold: 0.45, Sensitivity: 0.75, Specificity: 0.33, Accuracy: 0.68\n",
    "Threshold: 0.46, Sensitivity: 0.75, Specificity: 0.33, Accuracy: 0.68\n",
    "Threshold: 0.47, Sensitivity: 0.62, Specificity: 0.33, Accuracy: 0.58\n",
    "Threshold: 0.48, Sensitivity: 0.62, Specificity: 0.67, Accuracy: 0.63\n",
    "Threshold: 0.49, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.50, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.51, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.52, Sensitivity: 0.44, Specificity: 0.67, Accuracy: 0.47\n",
    "Threshold: 0.53, Sensitivity: 0.38, Specificity: 0.67, Accuracy: 0.42\n",
    "Threshold: 0.54, Sensitivity: 0.38, Specificity: 0.67, Accuracy: 0.42\n",
    "Threshold: 0.55, Sensitivity: 0.25, Specificity: 0.67, Accuracy: 0.32\n",
    "Threshold: 0.56, Sensitivity: 0.19, Specificity: 0.67, Accuracy: 0.26"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
