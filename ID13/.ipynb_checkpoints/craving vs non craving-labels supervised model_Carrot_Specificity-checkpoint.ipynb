{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fc5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa87750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"13.csv\")\n",
    "\n",
    "# Separate data into different dataframes based on data types\n",
    "spo2_data = data[data['data_type'] == 'spo2'].copy()\n",
    "heart_rate_data = data[data['data_type'] == 'hr'].copy()\n",
    "steps_data = data[data['data_type'] == 'steps'].copy()\n",
    "hrv_data = data[data['data_type'] == 'hrv'].copy()\n",
    "br_data = data[data['data_type'] == 'br'].copy()\n",
    "sleep_data = data[data['data_type'] == 'sleep'].copy()\n",
    "\n",
    "# Convert timestamps to datetime format for each dataframe\n",
    "spo2_data['time'] = pd.to_datetime(spo2_data['time'])\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time'])\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time'])\n",
    "hrv_data['time'] = pd.to_datetime(hrv_data['time'])\n",
    "br_data['time'] = pd.to_datetime(br_data['time'])\n",
    "sleep_data['time'] = pd.to_datetime(sleep_data['time'])\n",
    "\n",
    "# Sort dataframes based on time and their respective formats\n",
    "spo2_data.sort_values(by='time', inplace=True)\n",
    "heart_rate_data.sort_values(by='time', inplace=True)\n",
    "steps_data.sort_values(by='time', inplace=True)\n",
    "hrv_data.sort_values(by='time', inplace=True)\n",
    "br_data.sort_values(by='time', inplace=True)\n",
    "sleep_data.sort_values(by='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c06502",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data=heart_rate_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3062db4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76</td>\n",
       "      <td>2023-12-15 00:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>2023-12-15 00:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81</td>\n",
       "      <td>2023-12-15 00:02:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>78</td>\n",
       "      <td>2023-12-15 00:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>81</td>\n",
       "      <td>2023-12-15 00:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38689</th>\n",
       "      <td>86</td>\n",
       "      <td>2024-01-11 14:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38690</th>\n",
       "      <td>84</td>\n",
       "      <td>2024-01-11 14:51:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38691</th>\n",
       "      <td>82</td>\n",
       "      <td>2024-01-11 14:52:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38692</th>\n",
       "      <td>84</td>\n",
       "      <td>2024-01-11 14:53:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38693</th>\n",
       "      <td>89</td>\n",
       "      <td>2024-01-11 14:54:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24025 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "1        76 2023-12-15 00:00:00+00:00\n",
       "4        79 2023-12-15 00:01:00+00:00\n",
       "5        81 2023-12-15 00:02:00+00:00\n",
       "6        78 2023-12-15 00:03:00+00:00\n",
       "7        81 2023-12-15 00:04:00+00:00\n",
       "...     ...                       ...\n",
       "38689    86 2024-01-11 14:50:00+00:00\n",
       "38690    84 2024-01-11 14:51:00+00:00\n",
       "38691    82 2024-01-11 14:52:00+00:00\n",
       "38692    84 2024-01-11 14:53:00+00:00\n",
       "38693    89 2024-01-11 14:54:00+00:00\n",
       "\n",
       "[24025 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_rate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1f459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_data=steps_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293fadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-12-15 00:01:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-12-15 00:17:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-12-15 00:27:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>13</td>\n",
       "      <td>2023-12-15 00:43:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-12-15 00:46:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38715</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-11 21:29:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38716</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-11 21:54:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38717</th>\n",
       "      <td>14</td>\n",
       "      <td>2024-01-11 21:55:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38718</th>\n",
       "      <td>9</td>\n",
       "      <td>2024-01-11 23:41:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38719</th>\n",
       "      <td>34</td>\n",
       "      <td>2024-01-11 23:42:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6603 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "3         8 2023-12-15 00:01:00+00:00\n",
       "21        8 2023-12-15 00:17:00+00:00\n",
       "32        4 2023-12-15 00:27:00+00:00\n",
       "48       13 2023-12-15 00:43:00+00:00\n",
       "52       14 2023-12-15 00:46:00+00:00\n",
       "...     ...                       ...\n",
       "38715     2 2024-01-11 21:29:00+00:00\n",
       "38716     2 2024-01-11 21:54:00+00:00\n",
       "38717    14 2024-01-11 21:55:00+00:00\n",
       "38718     9 2024-01-11 23:41:00+00:00\n",
       "38719    34 2024-01-11 23:42:00+00:00\n",
       "\n",
       "[6603 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load craving and non-craving data\n",
    "craving_data = pd.read_csv('ID13_Crave.csv')\n",
    "non_craving_data = pd.read_csv('ID13_None.csv')\n",
    "\n",
    "# Filter rows with 'Melon' in the substance_fruit_label column\n",
    "craving_data = craving_data[craving_data['substance_fruit_label'] == 'Carrot']\n",
    "\n",
    "# Convert timestamps to ensure they are timezone-naive\n",
    "craving_data['hawaii_createdat_time'] = pd.to_datetime(craving_data['hawaii_createdat_time']).dt.tz_localize(None)\n",
    "non_craving_data['hawaii_createdat_time'] = pd.to_datetime(non_craving_data['hawaii_createdat_time']).dt.tz_localize(None)\n",
    "\n",
    "# Ensure heart rate and steps data use the same timezone-naive datetime index\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time']).dt.tz_localize(None)\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time']).dt.tz_localize(None)\n",
    "\n",
    "heart_rate_data.set_index('time', inplace=True)\n",
    "steps_data.set_index('time', inplace=True)\n",
    "\n",
    "# Reindex and forward fill to handle missing data\n",
    "heart_rate_data = heart_rate_data.reindex(pd.date_range(start=heart_rate_data.index.min(), end=heart_rate_data.index.max(), freq='min'), method='ffill')\n",
    "steps_data = steps_data.reindex(pd.date_range(start=steps_data.index.min(), end=steps_data.index.max(), freq='min'), method='ffill').fillna(0)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def process_label_data(label_data, heart_rate_data, steps_data, scaler):\n",
    "    results = []\n",
    "    for _, row in label_data.iterrows():\n",
    "        timestamp = row['hawaii_createdat_time']\n",
    "        lower_bound = timestamp - pd.Timedelta(hours=1)\n",
    "        upper_bound = timestamp + pd.Timedelta(hours=1)\n",
    "\n",
    "        # Extract data within the window\n",
    "        hr_window = heart_rate_data.loc[lower_bound:upper_bound]\n",
    "        steps_window = steps_data.loc[lower_bound:upper_bound]\n",
    "\n",
    "        # Resample and calculate mean every 4 minutes to get 30 points\n",
    "        hr_means = hr_window.resample('4min').mean().iloc[:30]  # Ensure 30 data points\n",
    "        steps_means = steps_window.resample('4min').mean().iloc[:30]\n",
    "\n",
    "        if len(hr_means) == 30 and len(steps_means) == 30:\n",
    "            # Standardize the means\n",
    "            hr_scaled = scaler.fit_transform(hr_means.values.reshape(-1, 1))\n",
    "            steps_scaled = scaler.fit_transform(steps_means.values.reshape(-1, 1))\n",
    "\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'heart_rate_means': hr_scaled.flatten().tolist(),\n",
    "                'steps_means': steps_scaled.flatten().tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process craving and non-craving labels\n",
    "craving_features = process_label_data(craving_data, heart_rate_data, steps_data, scaler)\n",
    "non_craving_features = process_label_data(non_craving_data, heart_rate_data, steps_data, scaler)\n",
    "\n",
    "# Output results\n",
    "print(\"Craving Features Data:\")\n",
    "print(craving_features.head())\n",
    "print(\"Non-Craving Features Data:\")\n",
    "print(non_craving_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the craving features data\n",
    "print(\"Shape of Craving Features Data:\", craving_features.shape)\n",
    "\n",
    "# Print the shape of the non-craving features data\n",
    "print(\"Shape of Non-Craving Features Data:\", non_craving_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adding label columns\n",
    "craving_features['state'] = 'crave'\n",
    "craving_features['state_val'] = 1\n",
    "\n",
    "non_craving_features['state'] = 'non_crave'\n",
    "non_craving_features['state_val'] = 0\n",
    "\n",
    "# Combining the dataframes\n",
    "combined_data = pd.concat([craving_features, non_craving_features])\n",
    "\n",
    "# Converting timestamps to datetime and sorting\n",
    "combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'])\n",
    "combined_data_sorted = combined_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# This combined_data_sorted is now ready for use in your neural network model\n",
    "print(combined_data_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aa308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained models\n",
    "encoder_hr = tf.keras.models.load_model('heart_rate_encoder')\n",
    "encoder_steps = tf.keras.models.load_model('steps_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d33482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to generate embeddings using the loaded models\n",
    "def generate_embeddings(hr_data, steps_data, encoder_hr, encoder_steps):\n",
    "    # Convert lists to numpy arrays and ensure they are in the correct shape\n",
    "    hr_data = np.array(hr_data.tolist())\n",
    "    steps_data = np.array(steps_data.tolist())\n",
    "\n",
    "    # Reshape data if required by the model, assuming the model expects shape (samples, features)\n",
    "    if hr_data.ndim == 1:\n",
    "        hr_data = hr_data.reshape(-1, 1)\n",
    "    if steps_data.ndim == 1:\n",
    "        steps_data = steps_data.reshape(-1, 1)\n",
    "\n",
    "    # Generate embeddings\n",
    "    hr_embeddings = encoder_hr.predict(hr_data)\n",
    "    steps_embeddings = encoder_steps.predict(steps_data)\n",
    "\n",
    "    return hr_embeddings, steps_embeddings\n",
    "\n",
    "# Use the function with your data\n",
    "hr_embeddings, steps_embeddings = generate_embeddings(\n",
    "    combined_data_sorted['heart_rate_means'],\n",
    "    combined_data_sorted['steps_means'],\n",
    "    encoder_hr,\n",
    "    encoder_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ffada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Combine the heart rate and steps embeddings\n",
    "combined_embeddings = np.concatenate([hr_embeddings, steps_embeddings], axis=1)\n",
    "\n",
    "# Labels from your existing DataFrame\n",
    "labels = combined_data_sorted['state_val'].values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with regularization and batch normalization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate thresholds on training set\n",
    "probabilities_train = model.predict(X_train)\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Store training results\n",
    "training_results = []\n",
    "\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_train > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    training_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert training results to DataFrame for easy viewing\n",
    "training_results_df = pd.DataFrame(training_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Select best threshold based on a specific criterion\n",
    "# Here, we select the threshold with the highest sensitivity while keeping specificity above 0.5\n",
    "best_threshold_row = training_results_df[(training_results_df['Sensitivity'] > 0.9) & (training_results_df['Specificity'] > 0.5)]\n",
    "if not best_threshold_row.empty:\n",
    "    best_threshold = best_threshold_row.iloc[0]['Threshold']\n",
    "else:\n",
    "    best_threshold = 0.5  # Default threshold if criteria is not met\n",
    "\n",
    "print(f\"\\nBest Threshold from Training Set: {best_threshold}\")\n",
    "\n",
    "# Evaluate the best threshold on the test set\n",
    "probabilities_test = model.predict(X_test)\n",
    "predicted_classes_test = (probabilities_test > best_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "recall_test = tp / (tp + fn)  # Sensitivity\n",
    "specificity_test = tn / (tn + fp)  # Specificity\n",
    "accuracy_test = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"\\nTest Set Evaluation at Best Threshold ({best_threshold}):\")\n",
    "print(f\"Sensitivity: {recall_test:.2f}\")\n",
    "print(f\"Specificity: {specificity_test:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.2f}\")\n",
    "\n",
    "# Store test results\n",
    "test_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_test > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    test_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert test results to DataFrame for easy viewing\n",
    "test_results_df = pd.DataFrame(test_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Find the threshold where specificity is around 0.9 and sensitivity is around 0.5\n",
    "desired_specificity = 0.9\n",
    "desired_sensitivity_range = (0.4, 0.6)  # Considering sensitivity around 0.5\n",
    "\n",
    "filtered_results = test_results_df[\n",
    "    (test_results_df['Specificity'] >= desired_specificity) & \n",
    "    (test_results_df['Sensitivity'] >= desired_sensitivity_range[0]) & \n",
    "    (test_results_df['Sensitivity'] <= desired_sensitivity_range[1])\n",
    "]\n",
    "\n",
    "# Print the filtered results\n",
    "print(\"\\nFiltered Thresholds with Specificity >= 0.9 and Sensitivity ~ 0.5:\")\n",
    "print(filtered_results)\n",
    "\n",
    "# Plot the results for better visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Sensitivity'], label='Sensitivity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Specificity'], label='Specificity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Accuracy'], label='Accuracy')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='Desired Specificity (0.9)')\n",
    "plt.axhline(y=0.5, color='g', linestyle='--', label='Desired Sensitivity (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity, Specificity, and Accuracy vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold: 0.48, Sensitivity: 0.58, Specificity: 0.78, Accuracy: 0.67\n",
    "Threshold: 0.48, Sensitivity: 0.75, Specificity: 1.00, Accuracy: 0.83"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
