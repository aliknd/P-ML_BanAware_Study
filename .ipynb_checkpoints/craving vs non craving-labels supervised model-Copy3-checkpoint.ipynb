{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fc5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa87750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"ID5/p5.csv\")\n",
    "\n",
    "# Separate data into different dataframes based on data types\n",
    "spo2_data = data[data['data_type'] == 'spo2'].copy()\n",
    "heart_rate_data = data[data['data_type'] == 'heart_rate'].copy()\n",
    "steps_data = data[data['data_type'] == 'steps'].copy()\n",
    "hrv_data = data[data['data_type'] == 'hrv'].copy()\n",
    "br_data = data[data['data_type'] == 'br'].copy()\n",
    "sleep_data = data[data['data_type'] == 'sleep'].copy()\n",
    "\n",
    "# Convert timestamps to datetime format for each dataframe\n",
    "spo2_data['time'] = pd.to_datetime(spo2_data['time'])\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time'])\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time'])\n",
    "hrv_data['time'] = pd.to_datetime(hrv_data['time'])\n",
    "br_data['time'] = pd.to_datetime(br_data['time'])\n",
    "sleep_data['time'] = pd.to_datetime(sleep_data['time'])\n",
    "\n",
    "# Sort dataframes based on time and their respective formats\n",
    "spo2_data.sort_values(by='time', inplace=True)\n",
    "heart_rate_data.sort_values(by='time', inplace=True)\n",
    "steps_data.sort_values(by='time', inplace=True)\n",
    "hrv_data.sort_values(by='time', inplace=True)\n",
    "br_data.sort_values(by='time', inplace=True)\n",
    "sleep_data.sort_values(by='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c06502",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data=heart_rate_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3062db4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8557</th>\n",
       "      <td>78.0</td>\n",
       "      <td>2023-11-14 22:34:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5146</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2023-11-14 22:35:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2023-11-14 22:36:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>85.0</td>\n",
       "      <td>2023-11-14 22:37:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8297</th>\n",
       "      <td>88.0</td>\n",
       "      <td>2023-11-14 22:38:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>89.0</td>\n",
       "      <td>2023-12-14 10:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>103.0</td>\n",
       "      <td>2023-12-14 10:05:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>95.0</td>\n",
       "      <td>2023-12-14 10:06:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>89.0</td>\n",
       "      <td>2023-12-14 10:07:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>84.0</td>\n",
       "      <td>2023-12-14 10:08:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7235 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "8557   78.0 2023-11-14 22:34:00+00:00\n",
       "5146   83.0 2023-11-14 22:35:00+00:00\n",
       "2119   83.0 2023-11-14 22:36:00+00:00\n",
       "8231   85.0 2023-11-14 22:37:00+00:00\n",
       "8297   88.0 2023-11-14 22:38:00+00:00\n",
       "...     ...                       ...\n",
       "6005   89.0 2023-12-14 10:04:00+00:00\n",
       "1120  103.0 2023-12-14 10:05:00+00:00\n",
       "4041   95.0 2023-12-14 10:06:00+00:00\n",
       "7456   89.0 2023-12-14 10:07:00+00:00\n",
       "2641   84.0 2023-12-14 10:08:00+00:00\n",
       "\n",
       "[7235 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_rate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1f459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_data=steps_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293fadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2023-11-14 17:09:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2023-11-14 22:35:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2023-11-14 22:36:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2023-11-14 22:52:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5752</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2023-11-14 22:54:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6263</th>\n",
       "      <td>26.0</td>\n",
       "      <td>2023-12-14 10:03:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2023-12-14 10:04:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>42.0</td>\n",
       "      <td>2023-12-14 10:05:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8768</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2023-12-14 10:06:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8312</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2023-12-14 10:08:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "1227    7.0 2023-11-14 17:09:00+00:00\n",
       "208     8.0 2023-11-14 22:35:00+00:00\n",
       "1372    7.0 2023-11-14 22:36:00+00:00\n",
       "8464    9.0 2023-11-14 22:52:00+00:00\n",
       "5752    6.0 2023-11-14 22:54:00+00:00\n",
       "...     ...                       ...\n",
       "6263   26.0 2023-12-14 10:03:00+00:00\n",
       "6641   75.0 2023-12-14 10:04:00+00:00\n",
       "2721   42.0 2023-12-14 10:05:00+00:00\n",
       "8768   18.0 2023-12-14 10:06:00+00:00\n",
       "8312   11.0 2023-12-14 10:08:00+00:00\n",
       "\n",
       "[1940 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load craving and non-craving data\n",
    "craving_data = pd.read_csv('ID5/ID5_Crave.csv')\n",
    "non_craving_data = pd.read_csv('ID5/ID5_None.csv')\n",
    "\n",
    "# Convert timestamps to ensure they are timezone-naive\n",
    "craving_data['hawaii_createdat_time'] = pd.to_datetime(craving_data['hawaii_createdat_time']).dt.tz_localize(None)\n",
    "non_craving_data['hawaii_createdat_time'] = pd.to_datetime(non_craving_data['hawaii_createdat_time']).dt.tz_localize(None)\n",
    "\n",
    "# Ensure heart rate and steps data use the same timezone-naive datetime index\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time']).dt.tz_localize(None)\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time']).dt.tz_localize(None)\n",
    "\n",
    "heart_rate_data.set_index('time', inplace=True)\n",
    "steps_data.set_index('time', inplace=True)\n",
    "\n",
    "# Reindex and forward fill to handle missing data\n",
    "heart_rate_data = heart_rate_data.reindex(pd.date_range(start=heart_rate_data.index.min(), end=heart_rate_data.index.max(), freq='min'), method='ffill')\n",
    "steps_data = steps_data.reindex(pd.date_range(start=steps_data.index.min(), end=steps_data.index.max(), freq='min'), method='ffill').fillna(0)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def process_label_data(label_data, heart_rate_data, steps_data, scaler):\n",
    "    results = []\n",
    "    for _, row in label_data.iterrows():\n",
    "        timestamp = row['hawaii_createdat_time']\n",
    "        lower_bound = timestamp - pd.Timedelta(hours=1)\n",
    "        upper_bound = timestamp + pd.Timedelta(hours=1)\n",
    "\n",
    "        # Extract data within the window\n",
    "        hr_window = heart_rate_data.loc[lower_bound:upper_bound]\n",
    "        steps_window = steps_data.loc[lower_bound:upper_bound]\n",
    "\n",
    "        # Resample and calculate mean every 4 minutes to get 30 points\n",
    "        hr_means = hr_window.resample('4min').mean().iloc[:30]  # Ensure 30 data points\n",
    "        steps_means = steps_window.resample('4min').mean().iloc[:30]\n",
    "\n",
    "        if len(hr_means) == 30 and len(steps_means) == 30:\n",
    "            # Standardize the means\n",
    "            hr_scaled = scaler.fit_transform(hr_means.values.reshape(-1, 1))\n",
    "            steps_scaled = scaler.fit_transform(steps_means.values.reshape(-1, 1))\n",
    "\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'heart_rate_means': hr_scaled.flatten().tolist(),\n",
    "                'steps_means': steps_scaled.flatten().tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process craving and non-craving labels\n",
    "craving_features = process_label_data(craving_data, heart_rate_data, steps_data, scaler)\n",
    "non_craving_features = process_label_data(non_craving_data, heart_rate_data, steps_data, scaler)\n",
    "\n",
    "# Output results\n",
    "print(\"Craving Features Data:\")\n",
    "print(craving_features.head())\n",
    "print(\"Non-Craving Features Data:\")\n",
    "print(non_craving_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the craving features data\n",
    "print(\"Shape of Craving Features Data:\", craving_features.shape)\n",
    "\n",
    "# Print the shape of the non-craving features data\n",
    "print(\"Shape of Non-Craving Features Data:\", non_craving_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming craving_features and non_craving_features are already defined as DataFrames\n",
    "\n",
    "# Adding label columns\n",
    "craving_features['state'] = 'crave'\n",
    "craving_features['state_val'] = 1\n",
    "\n",
    "non_craving_features['state'] = 'non_crave'\n",
    "non_craving_features['state_val'] = 0\n",
    "\n",
    "# Combining the dataframes\n",
    "combined_data = pd.concat([craving_features, non_craving_features])\n",
    "\n",
    "# Converting timestamps to datetime and sorting\n",
    "combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'])\n",
    "combined_data_sorted = combined_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# This combined_data_sorted is now ready for use in your neural network model\n",
    "print(combined_data_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aa308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained models\n",
    "encoder_hr = tf.keras.models.load_model('heart_rate_encoder')\n",
    "encoder_steps = tf.keras.models.load_model('steps_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d33482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to generate embeddings using the loaded models\n",
    "def generate_embeddings(hr_data, steps_data, encoder_hr, encoder_steps):\n",
    "    # Convert lists to numpy arrays and ensure they are in the correct shape\n",
    "    hr_data = np.array(hr_data.tolist())\n",
    "    steps_data = np.array(steps_data.tolist())\n",
    "\n",
    "    # Reshape data if required by the model, assuming the model expects shape (samples, features)\n",
    "    if hr_data.ndim == 1:\n",
    "        hr_data = hr_data.reshape(-1, 1)\n",
    "    if steps_data.ndim == 1:\n",
    "        steps_data = steps_data.reshape(-1, 1)\n",
    "\n",
    "    # Generate embeddings\n",
    "    hr_embeddings = encoder_hr.predict(hr_data)\n",
    "    steps_embeddings = encoder_steps.predict(steps_data)\n",
    "\n",
    "    return hr_embeddings, steps_embeddings\n",
    "\n",
    "# Use the function with your data\n",
    "hr_embeddings, steps_embeddings = generate_embeddings(\n",
    "    combined_data_sorted['heart_rate_means'],\n",
    "    combined_data_sorted['steps_means'],\n",
    "    encoder_hr,\n",
    "    encoder_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ffada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, classification_report, auc, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Combine the heart rate and steps embeddings\n",
    "combined_embeddings = np.concatenate([hr_embeddings, steps_embeddings], axis=1)\n",
    "\n",
    "# Labels from your existing DataFrame\n",
    "labels = combined_data_sorted['state_val'].values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with regularization and batch normalization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict probabilities\n",
    "probabilities = model.predict(X_test)\n",
    "thresholds = [0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes).ravel()\n",
    "    precision = precision_score(y_test, predicted_classes)\n",
    "    recall = recall_score(y_test, predicted_classes)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    print(f\"Threshold: {threshold:.2f}, TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}, \"\n",
    "          f\"Accuracy: {accuracy:.2%}, Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "# Evaluate the model on the test set and include all metrics\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]*100:.2f}%, Precision: {evaluation_results[2]}, Recall: {evaluation_results[3]}\")\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, probabilities)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Generate detailed classification report\n",
    "report = classification_report(y_test, predicted_classes, target_names=['Non-Craving', 'Craving'])\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate the ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probabilities)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate different thresholds\n",
    "for thresh in np.linspace(0, 1, 21):\n",
    "    predicted_classes = (probabilities > thresh).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes).ravel()\n",
    "    print(f\"Threshold: {thresh:.2f}, TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}, Accuracy: {(tp + tn) / (tp + tn + fp + fn):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cffed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
