{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fc5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa87750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"12.csv\")\n",
    "\n",
    "# Separate data into different dataframes based on data types\n",
    "spo2_data = data[data['data_type'] == 'spo2'].copy()\n",
    "heart_rate_data = data[data['data_type'] == 'hr'].copy()\n",
    "steps_data = data[data['data_type'] == 'steps'].copy()\n",
    "hrv_data = data[data['data_type'] == 'hrv'].copy()\n",
    "br_data = data[data['data_type'] == 'br'].copy()\n",
    "sleep_data = data[data['data_type'] == 'sleep'].copy()\n",
    "\n",
    "# Convert timestamps to datetime format for each dataframe\n",
    "spo2_data['time'] = pd.to_datetime(spo2_data['time'])\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time'])\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time'])\n",
    "hrv_data['time'] = pd.to_datetime(hrv_data['time'])\n",
    "br_data['time'] = pd.to_datetime(br_data['time'])\n",
    "sleep_data['time'] = pd.to_datetime(sleep_data['time'])\n",
    "\n",
    "# Sort dataframes based on time and their respective formats\n",
    "spo2_data.sort_values(by='time', inplace=True)\n",
    "heart_rate_data.sort_values(by='time', inplace=True)\n",
    "steps_data.sort_values(by='time', inplace=True)\n",
    "hrv_data.sort_values(by='time', inplace=True)\n",
    "br_data.sort_values(by='time', inplace=True)\n",
    "sleep_data.sort_values(by='time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c06502",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data=heart_rate_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3062db4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>2023-12-02 01:05:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77</td>\n",
       "      <td>2023-12-02 05:54:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>2023-12-02 05:55:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93</td>\n",
       "      <td>2023-12-02 05:56:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88</td>\n",
       "      <td>2023-12-02 05:57:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53320</th>\n",
       "      <td>104</td>\n",
       "      <td>2023-12-29 23:55:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53321</th>\n",
       "      <td>103</td>\n",
       "      <td>2023-12-29 23:56:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53322</th>\n",
       "      <td>104</td>\n",
       "      <td>2023-12-29 23:57:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53323</th>\n",
       "      <td>107</td>\n",
       "      <td>2023-12-29 23:58:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53324</th>\n",
       "      <td>106</td>\n",
       "      <td>2023-12-29 23:59:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34255 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "1        89 2023-12-02 01:05:00+00:00\n",
       "2        77 2023-12-02 05:54:00+00:00\n",
       "4        89 2023-12-02 05:55:00+00:00\n",
       "7        93 2023-12-02 05:56:00+00:00\n",
       "9        88 2023-12-02 05:57:00+00:00\n",
       "...     ...                       ...\n",
       "53320   104 2023-12-29 23:55:00+00:00\n",
       "53321   103 2023-12-29 23:56:00+00:00\n",
       "53322   104 2023-12-29 23:57:00+00:00\n",
       "53323   107 2023-12-29 23:58:00+00:00\n",
       "53324   106 2023-12-29 23:59:00+00:00\n",
       "\n",
       "[34255 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_rate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1f459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_data=steps_data[['value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293fadaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-12-02 01:05:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>2023-12-02 05:54:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>2023-12-02 05:55:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "      <td>2023-12-02 05:56:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>2023-12-02 05:57:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53245</th>\n",
       "      <td>46</td>\n",
       "      <td>2023-12-29 22:44:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53246</th>\n",
       "      <td>9</td>\n",
       "      <td>2023-12-29 22:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53248</th>\n",
       "      <td>7</td>\n",
       "      <td>2023-12-29 22:46:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53264</th>\n",
       "      <td>14</td>\n",
       "      <td>2023-12-29 23:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53266</th>\n",
       "      <td>21</td>\n",
       "      <td>2023-12-29 23:01:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                      time\n",
       "0         8 2023-12-02 01:05:00+00:00\n",
       "3        49 2023-12-02 05:54:00+00:00\n",
       "5        34 2023-12-02 05:55:00+00:00\n",
       "6        45 2023-12-02 05:56:00+00:00\n",
       "8        16 2023-12-02 05:57:00+00:00\n",
       "...     ...                       ...\n",
       "53245    46 2023-12-29 22:44:00+00:00\n",
       "53246     9 2023-12-29 22:45:00+00:00\n",
       "53248     7 2023-12-29 22:46:00+00:00\n",
       "53264    14 2023-12-29 23:00:00+00:00\n",
       "53266    21 2023-12-29 23:01:00+00:00\n",
       "\n",
       "[14461 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load use and non-use data\n",
    "use_data = pd.read_csv('ID27_Use.csv')\n",
    "non_use_data = pd.read_csv('ID27_None.csv')\n",
    "\n",
    "# Filter rows with 'Melon' in the substance_fruit_label column\n",
    "use_data = use_data[use_data['substance_fruit_label'] == 'Melon']\n",
    "\n",
    "# Standardize the timestamp format in 'hawaii_use_time'\n",
    "use_data['hawaii_use_time'] = use_data['hawaii_use_time'].apply(lambda x: x.split('.')[0] if '.' in x else x)\n",
    "use_data['hawaii_use_time'] = pd.to_datetime(use_data['hawaii_use_time'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)\n",
    "\n",
    "# Convert timestamps to ensure they are timezone-naive for non-use data\n",
    "non_use_data['hawaii_createdat_time'] = pd.to_datetime(non_use_data['hawaii_createdat_time'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(None)\n",
    "\n",
    "# Ensure heart rate and steps data use the same timezone-naive datetime index\n",
    "heart_rate_data['time'] = pd.to_datetime(heart_rate_data['time']).dt.tz_localize(None)\n",
    "steps_data['time'] = pd.to_datetime(steps_data['time']).dt.tz_localize(None)\n",
    "\n",
    "heart_rate_data.set_index('time', inplace=True)\n",
    "steps_data.set_index('time', inplace=True)\n",
    "\n",
    "# Convert the 'value' column to numeric, coercing errors\n",
    "heart_rate_data['value'] = pd.to_numeric(heart_rate_data['value'], errors='coerce')\n",
    "steps_data['value'] = pd.to_numeric(steps_data['value'], errors='coerce')\n",
    "\n",
    "# Remove duplicate timestamps\n",
    "heart_rate_data = heart_rate_data[~heart_rate_data.index.duplicated(keep='first')]\n",
    "steps_data = steps_data[~steps_data.index.duplicated(keep='first')]\n",
    "\n",
    "# Reindex and forward fill to handle missing data\n",
    "heart_rate_data = heart_rate_data.reindex(pd.date_range(start=heart_rate_data.index.min(), end=heart_rate_data.index.max(), freq='min'), method='ffill')\n",
    "steps_data = steps_data.reindex(pd.date_range(start=steps_data.index.min(), end=steps_data.index.max(), freq='min'), method='ffill').fillna(0)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def process_label_data(label_data, heart_rate_data, steps_data, scaler, time_column):\n",
    "    results = []\n",
    "    for _, row in label_data.iterrows():\n",
    "        timestamp = row[time_column]\n",
    "        lower_bound = timestamp - pd.Timedelta(hours=1)\n",
    "        upper_bound = timestamp + pd.Timedelta(hours=1)\n",
    "\n",
    "        # Extract data within the window\n",
    "        hr_window = heart_rate_data.loc[lower_bound:upper_bound]\n",
    "        steps_window = steps_data.loc[lower_bound:upper_bound]\n",
    "\n",
    "        # Resample and calculate mean every 4 minutes to get 30 points\n",
    "        hr_means = hr_window.resample('4min').mean().iloc[:30]  # Ensure 30 data points\n",
    "        steps_means = steps_window.resample('4min').mean().iloc[:30]\n",
    "\n",
    "        if len(hr_means) == 30 and len(steps_means) == 30:\n",
    "            # Standardize the means\n",
    "            hr_scaled = scaler.fit_transform(hr_means.values.reshape(-1, 1))\n",
    "            steps_scaled = scaler.fit_transform(steps_means.values.reshape(-1, 1))\n",
    "\n",
    "            results.append({\n",
    "                'timestamp': timestamp,\n",
    "                'heart_rate_means': hr_scaled.flatten().tolist(),\n",
    "                'steps_means': steps_scaled.flatten().tolist()\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process use and non-use labels\n",
    "use_features = process_label_data(use_data, heart_rate_data, steps_data, scaler, 'hawaii_use_time')\n",
    "non_use_features = process_label_data(non_use_data, heart_rate_data, steps_data, scaler, 'hawaii_createdat_time')\n",
    "\n",
    "# Output results\n",
    "print(\"Use Features Data:\")\n",
    "print(use_features.head())\n",
    "print(\"Non-Use Features Data:\")\n",
    "print(non_use_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the use features data\n",
    "print(\"Shape of Use Features Data:\", use_features.shape)\n",
    "\n",
    "# Print the shape of the non-use features data\n",
    "print(\"Shape of Non-Use Features Data:\", non_use_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adding label columns\n",
    "use_features['state'] = 'use'\n",
    "use_features['state_val'] = 1\n",
    "\n",
    "non_use_features['state'] = 'non_crave'\n",
    "non_use_features['state_val'] = 0\n",
    "\n",
    "# Combining the dataframes\n",
    "combined_data = pd.concat([use_features, non_use_features])\n",
    "\n",
    "# Converting timestamps to datetime and sorting\n",
    "combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'])\n",
    "combined_data_sorted = combined_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "# This combined_data_sorted is now ready for use in your neural network model\n",
    "print(combined_data_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aa308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load pretrained models\n",
    "encoder_hr = tf.keras.models.load_model('heart_rate_encoder')\n",
    "encoder_steps = tf.keras.models.load_model('steps_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d33482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to generate embeddings using the loaded models\n",
    "def generate_embeddings(hr_data, steps_data, encoder_hr, encoder_steps):\n",
    "    # Convert lists to numpy arrays and ensure they are in the correct shape\n",
    "    hr_data = np.array(hr_data.tolist())\n",
    "    steps_data = np.array(steps_data.tolist())\n",
    "\n",
    "    # Reshape data if required by the model, assuming the model expects shape (samples, features)\n",
    "    if hr_data.ndim == 1:\n",
    "        hr_data = hr_data.reshape(-1, 1)\n",
    "    if steps_data.ndim == 1:\n",
    "        steps_data = steps_data.reshape(-1, 1)\n",
    "\n",
    "    # Generate embeddings\n",
    "    hr_embeddings = encoder_hr.predict(hr_data)\n",
    "    steps_embeddings = encoder_steps.predict(steps_data)\n",
    "\n",
    "    return hr_embeddings, steps_embeddings\n",
    "\n",
    "# Use the function with your data\n",
    "hr_embeddings, steps_embeddings = generate_embeddings(\n",
    "    combined_data_sorted['heart_rate_means'],\n",
    "    combined_data_sorted['steps_means'],\n",
    "    encoder_hr,\n",
    "    encoder_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ffada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Combine the heart rate and steps embeddings\n",
    "combined_embeddings = np.concatenate([hr_embeddings, steps_embeddings], axis=1)\n",
    "\n",
    "# Labels from your existing DataFrame\n",
    "labels = combined_data_sorted['state_val'].values\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with regularization and batch normalization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate thresholds on training set\n",
    "probabilities_train = model.predict(X_train)\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Store training results\n",
    "training_results = []\n",
    "\n",
    "print(\"\\nTraining Set Evaluation:\")\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_train > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    training_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert training results to DataFrame for easy viewing\n",
    "training_results_df = pd.DataFrame(training_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Select best threshold based on a specific criterion\n",
    "# Here, we select the threshold with the highest sensitivity while keeping specificity above 0.5\n",
    "best_threshold_row = training_results_df[(training_results_df['Sensitivity'] > 0.9) & (training_results_df['Specificity'] > 0.5)]\n",
    "if not best_threshold_row.empty:\n",
    "    best_threshold = best_threshold_row.iloc[0]['Threshold']\n",
    "else:\n",
    "    best_threshold = 0.5  # Default threshold if criteria is not met\n",
    "\n",
    "print(f\"\\nBest Threshold from Training Set: {best_threshold}\")\n",
    "\n",
    "# Evaluate the best threshold on the test set\n",
    "probabilities_test = model.predict(X_test)\n",
    "predicted_classes_test = (probabilities_test > best_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes_test).ravel()\n",
    "recall_test = tp / (tp + fn)  # Sensitivity\n",
    "specificity_test = tn / (tn + fp)  # Specificity\n",
    "accuracy_test = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(f\"\\nTest Set Evaluation at Best Threshold ({best_threshold}):\")\n",
    "print(f\"Sensitivity: {recall_test:.2f}\")\n",
    "print(f\"Specificity: {specificity_test:.2f}\")\n",
    "print(f\"Accuracy: {accuracy_test:.2f}\")\n",
    "\n",
    "# Store test results\n",
    "test_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    predicted_classes = (probabilities_test > threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predicted_classes).ravel()\n",
    "    recall = tp / (tp + fn)  # Sensitivity\n",
    "    specificity = tn / (tn + fp)  # Specificity\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    test_results.append((threshold, recall, specificity, accuracy))\n",
    "    print(f\"Threshold: {threshold:.2f}, Sensitivity: {recall:.2f}, Specificity: {specificity:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert test results to DataFrame for easy viewing\n",
    "test_results_df = pd.DataFrame(test_results, columns=['Threshold', 'Sensitivity', 'Specificity', 'Accuracy'])\n",
    "\n",
    "# Find the threshold where specificity is around 0.9 and sensitivity is around 0.5\n",
    "desired_specificity = 0.9\n",
    "desired_sensitivity_range = (0.4, 0.6)  # Considering sensitivity around 0.5\n",
    "\n",
    "filtered_results = test_results_df[\n",
    "    (test_results_df['Specificity'] >= desired_specificity) & \n",
    "    (test_results_df['Sensitivity'] >= desired_sensitivity_range[0]) & \n",
    "    (test_results_df['Sensitivity'] <= desired_sensitivity_range[1])\n",
    "]\n",
    "\n",
    "# Print the filtered results\n",
    "print(\"\\nFiltered Thresholds with Specificity >= 0.9 and Sensitivity ~ 0.5:\")\n",
    "print(filtered_results)\n",
    "\n",
    "# Plot the results for better visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Sensitivity'], label='Sensitivity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Specificity'], label='Specificity')\n",
    "plt.plot(test_results_df['Threshold'], test_results_df['Accuracy'], label='Accuracy')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='Desired Specificity (0.9)')\n",
    "plt.axhline(y=0.5, color='g', linestyle='--', label='Desired Sensitivity (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Sensitivity, Specificity, and Accuracy vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb2d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training\n",
    "Threshold: 0.45, Sensitivity: 0.90, Specificity: 0.57, Accuracy: 0.80\n",
    "Threshold: 0.46, Sensitivity: 0.87, Specificity: 0.83, Accuracy: 0.85\n",
    "Threshold: 0.47, Sensitivity: 0.75, Specificity: 0.87, Accuracy: 0.79\n",
    "Threshold: 0.48, Sensitivity: 0.73, Specificity: 0.96, Accuracy: 0.80\n",
    "Threshold: 0.49, Sensitivity: 0.69, Specificity: 0.96, Accuracy: 0.77\n",
    "Threshold: 0.50, Sensitivity: 0.67, Specificity: 0.96, Accuracy: 0.76\n",
    "Threshold: 0.51, Sensitivity: 0.65, Specificity: 0.96, Accuracy: 0.75\n",
    "Threshold: 0.52, Sensitivity: 0.62, Specificity: 0.96, Accuracy: 0.72\n",
    "Threshold: 0.53, Sensitivity: 0.62, Specificity: 0.96, Accuracy: 0.72\n",
    "Threshold: 0.54, Sensitivity: 0.56, Specificity: 1.00, Accuracy: 0.69\n",
    "Threshold: 0.55, Sensitivity: 0.48, Specificity: 1.00, Accuracy: 0.64\n",
    "Threshold: 0.56, Sensitivity: 0.44, Specificity: 1.00, Accuracy: 0.61\n",
    "Test:\n",
    "Threshold: 0.45, Sensitivity: 0.75, Specificity: 0.33, Accuracy: 0.68\n",
    "Threshold: 0.46, Sensitivity: 0.75, Specificity: 0.33, Accuracy: 0.68\n",
    "Threshold: 0.47, Sensitivity: 0.62, Specificity: 0.33, Accuracy: 0.58\n",
    "Threshold: 0.48, Sensitivity: 0.62, Specificity: 0.67, Accuracy: 0.63\n",
    "Threshold: 0.49, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.50, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.51, Sensitivity: 0.56, Specificity: 0.67, Accuracy: 0.58\n",
    "Threshold: 0.52, Sensitivity: 0.44, Specificity: 0.67, Accuracy: 0.47\n",
    "Threshold: 0.53, Sensitivity: 0.38, Specificity: 0.67, Accuracy: 0.42\n",
    "Threshold: 0.54, Sensitivity: 0.38, Specificity: 0.67, Accuracy: 0.42\n",
    "Threshold: 0.55, Sensitivity: 0.25, Specificity: 0.67, Accuracy: 0.32\n",
    "Threshold: 0.56, Sensitivity: 0.19, Specificity: 0.67, Accuracy: 0.26"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
